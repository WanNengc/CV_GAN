# GAN Learning Project

This is a project for learning Generative Adversarial Networks (GANs), including the traditional DCGAN and an upgraded version of GAN inspired by StyleGAN. The project also provides SEFA (Semantic Factorization) scripts for both GAN models, which are used for factor analysis in the latent space and image generation.

## Table of Contents

- [Project Overview](#project-overview)
- [Features](#features)
- [Directory Structure](#directory-structure)
- [Installation](#installation)
- [Usage](#usage)
  - [Traditional DCGAN](#traditional-dcgan)
    - [Training DCGAN](#training-dcgan)
    - [Running SEFA Script](#running-sefa-script)
  - [Upgraded GAN (Inspired by StyleGAN)](#upgraded-gan-inspired-by-stylegan)
    - [Training Upgraded GAN](#training-upgraded-gan)
    - [Running SEFA Script](#running-sefa-script-1)
  - [Evaluating the Generator](#evaluating-the-generator)
- [Detailed SEFA Method](#detailed-sefa-method)
- [Adv_SeFa Visualization Tool User Guide](#adv_sefa-visualization-tool-user-guide)
  - [Streamlit Visualization](#streamlit-visualization)
- [Contributing](#contributing)

## Project Overview

This project aims to delve deep into studying and practicing various architectures and techniques of Generative Adversarial Networks (GANs). The project includes the following main components:

1. **Traditional DCGAN**: Implements a simplified Deep Convolutional GAN for image generation.
2. **Upgraded GAN (Inspired by StyleGAN)**: Based on the traditional DCGAN, inspired by StyleGAN, it incorporates a mapping network and Adaptive Instance Normalization (AdaIN) layers to enhance the quality and diversity of generated images.
3. **SEFA Scripts**: Provides SEFA (Semantic Factorization) scripts for DCGAN to perform factor analysis in the latent space and image generation.

## Features

- **Traditional DCGAN**:
  - Basic generator and discriminator design
  - Trained using WGAN loss with gradient penalty
  - Supports saving generated images and creating GIF animations

- **Upgraded GAN**:
  - Introduces Mapping Network and Adaptive Instance Normalization (AdaIN) layers
  - Generates higher quality images
  - Supports latent vector manipulation in W space

- **SEFA Scripts**:
  - Performs factor analysis on the generator's weights
  - Discovers and manipulates semantic directions in the latent space
  - Provides Streamlit visualization tools

## Directory Structure

| Directory              | Description                                     |
|------------------------|-------------------------------------------------|
| `adv_gif_results/`     | GIF animation results generated by the upgraded GAN |
| `adv_results/`         | Image results generated by the upgraded GAN      |
| `adv_snapshots/`       | Model snapshots of the upgraded GAN              |
| `face/face/`           | Image dataset                                    |
| `gif_results/`         | GIF animation results generated by the traditional DCGAN |
| `results/`             | Image results generated by the traditional DCGAN |
| `sefa_result/`         | SEFA analysis results                            |
| `snapshots/`           | Model snapshots of the traditional DCGAN         |
| `Adv_SeFa.py`          | Streamlit visualization application for SEFA script |
| `GAN.py`               | Training and generation script for traditional DCGAN |
| `New_GAN.py`           | Training and generation script for the upgraded GAN |
| `evaluate_gan.py`      | Script to evaluate generator performance          |
| `sefa.py`              | SEFA analysis script for traditional DCGAN         |
| `requirements.txt`     | List of project dependencies                       |
| `README.md`            | Project documentation                              |

## Installation

### Requirements

- Python 3.7+
- PyTorch 2.5.1+
- Other dependencies listed in `requirements.txt`

### Installation

1. **Install dependencies**
    ```bash
    pip install -r requirements.txt
    ```

## Usage

### Traditional DCGAN

#### Training DCGAN

1. **Prepare the dataset**

    Place your image dataset in the `face/` directory, ensuring the dataset is organized according to PyTorch's `ImageFolder` format.

2. **Run the training script**
    ```bash
    python GAN.py
    ```
    During training, generated images will be saved in the `results/` directory, training model weights will be saved in the `snapshots/` directory, and generated GIF animations will be saved in the `gif_results/` directory.

#### Running SEFA Script

1. **Ensure DCGAN is trained and generator weights are saved**

    By default, generator weights are saved in `snapshots/gnet.pth`.

2. **Run the SEFA script**
    ```bash
    python sefa.py
    ```

    The SEFA script will perform factor analysis on the generator's weights and generate HTML visualization results, saved in the `sefa_result/` directory.

### Upgraded GAN (Inspired by StyleGAN)

#### Training Upgraded GAN

1. **Prepare the dataset**

    Place your image dataset in the `face/` directory, ensuring the dataset is organized according to PyTorch's `ImageFolder` format.

2. **Run the training script**
    ```bash
    python New_GAN.py
    ```
    During training, generated images will be saved in the `adv_results/` directory, training model weights will be saved in the `adv_snapshots/` directory, and generated GIF animations will be saved in the `adv_gif_results/` directory.

#### Running SEFA Script

##### Streamlit Visualization

```bash
streamlit run Adv_SeFa.py
```
Open the browser and visit the provided local address to perform SEFA analysis and image generation through an interactive interface.

### Evaluating the Generator

This project provides the `evaluate_gan.py` script to evaluate multiple trained GAN models, calculating Inception Score (IS), Frechet Inception Distance (FID), and Kernel Inception Distance (KID).

#### Explanation of Evaluation Metrics

- **Inception Score (IS)**:
  - Measures the quality and diversity of generated images.
  - Higher scores indicate greater diversity across categories and better image quality.

- **Frechet Inception Distance (FID)**:
  - Measures the distance between generated images and real images.
  - Lower scores are better, indicating that the distribution of generated images is closer to that of real images.

- **Kernel Inception Distance (KID)**:
  - Another metric for evaluating image quality, similar to FID.
  - Lower scores are better, indicating smaller distances between generated images and real images in the feature space.

#### Running the Evaluation Script

1. **Ensure that the generator weights are trained and saved**

    Generator weights should be saved in the `snapshots/` or `adv_snapshots/` directory.

2. **Run the evaluation script**
    ```bash
    python evaluate_gan.py \
        --generators "GAN.Gnet:snapshots/gnet.pth:generated_images_gan" \
                     "New_GAN.AdvancedGnet:adv_snapshots/adv_gnet.pth:generated_images_new_gan" \
        --real_images face/face/ \
        --num_images 1000 \
        --batch_size 32 \
        --device cuda
    ```

    **Parameter Descriptions**:
    
    - `--generators`: List of generator specifications in the format `"module.class:path_to_weights:generated_dir"`.
      - `module.class`: The module and class name of the generator, such as `GAN.Gnet` or `New_GAN.AdvancedGnet`.
      - `path_to_weights`: Path to the generator weights file.
      - `generated_dir`: Directory where generated images will be saved.
    - `--real_images`: Path to the folder containing real images.
    - `--num_images`: Number of images to generate for evaluation.
    - `--batch_size`: Batch size.
    - `--device`: Device to use (`cuda` or `cpu`).

    **Example**:
    
    ```bash
    python evaluate_gan.py --generators "GAN.Gnet:snapshots/gnet.pth:generated_images_gan" "New_GAN.AdvancedGnet:adv_snapshots/adv_gnet.pth:generated_images_new_gan" --real_images face/face/ --num_images 1000 --batch_size 32 --device cuda
    ```

    The script will generate the specified number of images and save them in the `generated_images_gan` and `generated_images_new_gan` directories, then calculate Inception Score, FID, and KID, and output the evaluation results.

## Detailed SEFA Method

### What is SEFA?

**SEFA** (Semantic Factorization) is a method for analyzing and manipulating the latent space of Generative Adversarial Networks (GANs). SEFA uses factor analysis techniques to identify the main factors in the latent space, enabling semantic-level manipulation of generated images. This method was proposed in a [relevant paper](https://arxiv.org/abs/2007.06600) in 2020, aiming to enhance the interpretability and controllability of GAN models.

### Core Ideas of SEFA

1. **Latent Space Analysis**:
   SEFA analyzes the GAN's latent space, especially the mapped `w` space, and identifies latent semantic factors through factor analysis. These factors represent different semantic attributes in the generated images, such as facial expressions, poses, backgrounds, etc.

2. **Factor Manipulation**:
   The identified factors can be used to manipulate the generator's output. For example, by adjusting the weights of specific factors, you can increase or decrease certain features in the generated image, such as the intensity of a smile or the tilt of the head.

3. **Visualization**:
   SEFA provides visualization tools to help users intuitively understand and operate these factors. Through an interactive interface, users can adjust factor weights in real-time and observe changes in the generated images, thereby gaining deeper insights into the structure of the GAN's latent space.

### Advantages of SEFA

- **Interpretability**:
  SEFA offers a systematic approach to understanding different dimensions in the GAN's latent space and their corresponding semantic meanings.

- **Controllability**:
  Through factor analysis, users can precisely manipulate specific attributes of the generated images, achieving higher levels of control over image generation.

- **Usability**:
  Combined with visualization tools like Streamlit, SEFA makes complex latent space operations intuitive and user-friendly, allowing even non-expert users to easily utilize it.

### Applications of SEFA

In this project, SEFA is applied to:

- **Latent Vector Analysis**:
  Collecting a large number of latent vectors and extracting major factors through factor analysis to reveal key semantic dimensions in the latent space.

- **Semantic Manipulation**:
  Adjusting the directions of latent vectors based on the identified factors to achieve semantic-level control of generated images, such as altering facial expressions, poses, or other features.

- **Result Visualization**:
  Providing interactive tools to help users understand and apply these factors, observing the impact of factor adjustments on generated images in real-time.

## Adv_SeFa Visualization Tool User Guide

**Adv_SeFa** provides an interactive visualization tool based on **Streamlit** for displaying and manipulating the results of SEFA analysis. Below are detailed instructions on how to use this tool.

### Streamlit Visualization

**Streamlit** is a Python library for building data applications, allowing you to quickly create interactive web applications.

#### Running the Streamlit Application

1. **Ensure Streamlit is installed**

   If not installed, you can install it via `requirements.txt`, or install it separately:

    ```bash
    pip install streamlit
    ```

2. **Run the SEFA script**

   In the project root directory, run the following command:

    ```bash
    streamlit run Adv_SeFa.py
    ```

3. **Access the application**

    After running the command, the terminal will display a local address (e.g., http://localhost:8501). Open your browser and navigate to that address to use the SEFA visualization tool.

#### Using the Streamlit Application

The application interface mainly includes the following sections:

1. **Parameter Configuration** (Sidebar):
   - **Number of factors to analyze**: Select the number of factors to analyze.
   - **Number of top factors to visualize**: Select the top number of factors to visualize.
   - **Adjustment magnitude**: The magnitude of factor direction adjustment.

2. **Main Interface**:
   - **Device Information**: Displays the device in use (e.g., CUDA or CPU).
   - **Progress Indicator**: Shows the progress of latent vector collection and factor analysis.
   - **Generated Image Display**: Shows manipulated images and the effects of factors.
   - **Download Results**: Provides a button to allow users to download the generated SEFA analysis results.

3. **Operation Steps**:

   - **Adjust Factors**: Adjust the direction and magnitude of each factor using sliders.
   - **Generate Images**: Generate corresponding images based on the adjusted factors.
   - **View and Download Results**: View the visualized images and optionally download the analysis results.

#### Example Operation

1. **Start the application**:
   - Run `streamlit run Adv_SeFa.py`.
   - Open the browser and visit the provided local address.

2. **Configure Parameters**:
   - In the sidebar, adjust **Number of factors to analyze**, **Number of top factors to visualize**, and **Adjustment magnitude**.

3. **Perform Analysis**:
   - The application will automatically start collecting latent vectors, performing factor analysis, and displaying the effects of the top `top_k` factors.

4. **View Results**:
   - In the main interface, view the manipulated images to understand the impact of different factors on the generated images.

5. **Download Results**:
   - Click the **Download Results** button to download the generated SEFA analysis results as a compressed file.

### Notes

- **Performance Requirements**:
  - SEFA analysis and image generation processes may require substantial computational resources. It is recommended to run in an environment with GPU support to accelerate processing.

- **Data Preparation**:
  - Ensure that the GAN models are successfully trained and corresponding weight files are generated before running the SEFA scripts.

- **File Paths**:
  - Ensure that the paths in the configuration (e.g., `adv_g_net_path`) correctly point to the pre-trained models and result saving directories.

## Contributing


---

Thank you for your attention and support! We hope this project helps you better understand and master various technologies and applications of Generative Adversarial Networks (GANs).
